{"ast":null,"code":"import { assign, uuid, rev, invalidIdError, normalizeDdocFunctionName, parseDdocFunctionName } from 'pouchdb-utils';\nexport { invalidIdError, normalizeDdocFunctionName, parseDdocFunctionName } from 'pouchdb-utils';\nimport { DOC_VALIDATION, INVALID_REV, createError, BAD_ARG, REV_CONFLICT, MISSING_DOC } from 'pouchdb-errors';\nimport { atob, btoa, binaryStringToBlobOrBuffer, blobOrBufferToBinaryString, blobOrBufferToBase64 } from 'pouchdb-binary-utils';\nimport { binaryMd5 } from 'pouchdb-md5';\nimport { isDeleted, merge, winningRev, revExists, isLocalId } from 'pouchdb-merge';\nexport { isDeleted, isLocalId } from 'pouchdb-merge';\nimport { Map } from 'pouchdb-collections';\n\nfunction allDocsKeysQuery(api, opts) {\n  var keys = opts.keys;\n  var finalResults = {\n    offset: opts.skip\n  };\n  return Promise.all(keys.map(function (key) {\n    var subOpts = assign({\n      key: key,\n      deleted: 'ok'\n    }, opts);\n    ['limit', 'skip', 'keys'].forEach(function (optKey) {\n      delete subOpts[optKey];\n    });\n    return new Promise(function (resolve, reject) {\n      api._allDocs(subOpts, function (err, res) {\n        /* istanbul ignore if */\n        if (err) {\n          return reject(err);\n        }\n        /* istanbul ignore if */\n\n\n        if (opts.update_seq && res.update_seq !== undefined) {\n          finalResults.update_seq = res.update_seq;\n        }\n\n        finalResults.total_rows = res.total_rows;\n        resolve(res.rows[0] || {\n          key: key,\n          error: 'not_found'\n        });\n      });\n    });\n  })).then(function (results) {\n    finalResults.rows = results;\n    return finalResults;\n  });\n}\n\nfunction toObject(array) {\n  return array.reduce(function (obj, item) {\n    obj[item] = true;\n    return obj;\n  }, {});\n} // List of top level reserved words for doc\n\n\nvar reservedWords = toObject(['_id', '_rev', '_attachments', '_deleted', '_revisions', '_revs_info', '_conflicts', '_deleted_conflicts', '_local_seq', '_rev_tree', //replication documents\n'_replication_id', '_replication_state', '_replication_state_time', '_replication_state_reason', '_replication_stats', // Specific to Couchbase Sync Gateway\n'_removed']); // List of reserved words that should end up the document\n\nvar dataWords = toObject(['_attachments', //replication documents\n'_replication_id', '_replication_state', '_replication_state_time', '_replication_state_reason', '_replication_stats']);\n\nfunction parseRevisionInfo(rev$$1) {\n  if (!/^\\d+-./.test(rev$$1)) {\n    return createError(INVALID_REV);\n  }\n\n  var idx = rev$$1.indexOf('-');\n  var left = rev$$1.substring(0, idx);\n  var right = rev$$1.substring(idx + 1);\n  return {\n    prefix: parseInt(left, 10),\n    id: right\n  };\n}\n\nfunction makeRevTreeFromRevisions(revisions, opts) {\n  var pos = revisions.start - revisions.ids.length + 1;\n  var revisionIds = revisions.ids;\n  var ids = [revisionIds[0], opts, []];\n\n  for (var i = 1, len = revisionIds.length; i < len; i++) {\n    ids = [revisionIds[i], {\n      status: 'missing'\n    }, [ids]];\n  }\n\n  return [{\n    pos: pos,\n    ids: ids\n  }];\n} // Preprocess documents, parse their revisions, assign an id and a\n// revision for new writes that are missing them, etc\n\n\nfunction parseDoc(doc, newEdits, dbOpts) {\n  if (!dbOpts) {\n    dbOpts = {\n      deterministic_revs: true\n    };\n  }\n\n  var nRevNum;\n  var newRevId;\n  var revInfo;\n  var opts = {\n    status: 'available'\n  };\n\n  if (doc._deleted) {\n    opts.deleted = true;\n  }\n\n  if (newEdits) {\n    if (!doc._id) {\n      doc._id = uuid();\n    }\n\n    newRevId = rev(doc, dbOpts.deterministic_revs);\n\n    if (doc._rev) {\n      revInfo = parseRevisionInfo(doc._rev);\n\n      if (revInfo.error) {\n        return revInfo;\n      }\n\n      doc._rev_tree = [{\n        pos: revInfo.prefix,\n        ids: [revInfo.id, {\n          status: 'missing'\n        }, [[newRevId, opts, []]]]\n      }];\n      nRevNum = revInfo.prefix + 1;\n    } else {\n      doc._rev_tree = [{\n        pos: 1,\n        ids: [newRevId, opts, []]\n      }];\n      nRevNum = 1;\n    }\n  } else {\n    if (doc._revisions) {\n      doc._rev_tree = makeRevTreeFromRevisions(doc._revisions, opts);\n      nRevNum = doc._revisions.start;\n      newRevId = doc._revisions.ids[0];\n    }\n\n    if (!doc._rev_tree) {\n      revInfo = parseRevisionInfo(doc._rev);\n\n      if (revInfo.error) {\n        return revInfo;\n      }\n\n      nRevNum = revInfo.prefix;\n      newRevId = revInfo.id;\n      doc._rev_tree = [{\n        pos: nRevNum,\n        ids: [newRevId, opts, []]\n      }];\n    }\n  }\n\n  invalidIdError(doc._id);\n  doc._rev = nRevNum + '-' + newRevId;\n  var result = {\n    metadata: {},\n    data: {}\n  };\n\n  for (var key in doc) {\n    /* istanbul ignore else */\n    if (Object.prototype.hasOwnProperty.call(doc, key)) {\n      var specialKey = key[0] === '_';\n\n      if (specialKey && !reservedWords[key]) {\n        var error = createError(DOC_VALIDATION, key);\n        error.message = DOC_VALIDATION.message + ': ' + key;\n        throw error;\n      } else if (specialKey && !dataWords[key]) {\n        result.metadata[key.slice(1)] = doc[key];\n      } else {\n        result.data[key] = doc[key];\n      }\n    }\n  }\n\n  return result;\n}\n\nfunction parseBase64(data) {\n  try {\n    return atob(data);\n  } catch (e) {\n    var err = createError(BAD_ARG, 'Attachment is not a valid base64 string');\n    return {\n      error: err\n    };\n  }\n}\n\nfunction preprocessString(att, blobType, callback) {\n  var asBinary = parseBase64(att.data);\n\n  if (asBinary.error) {\n    return callback(asBinary.error);\n  }\n\n  att.length = asBinary.length;\n\n  if (blobType === 'blob') {\n    att.data = binaryStringToBlobOrBuffer(asBinary, att.content_type);\n  } else if (blobType === 'base64') {\n    att.data = btoa(asBinary);\n  } else {\n    // binary\n    att.data = asBinary;\n  }\n\n  binaryMd5(asBinary, function (result) {\n    att.digest = 'md5-' + result;\n    callback();\n  });\n}\n\nfunction preprocessBlob(att, blobType, callback) {\n  binaryMd5(att.data, function (md5) {\n    att.digest = 'md5-' + md5; // size is for blobs (browser), length is for buffers (node)\n\n    att.length = att.data.size || att.data.length || 0;\n\n    if (blobType === 'binary') {\n      blobOrBufferToBinaryString(att.data, function (binString) {\n        att.data = binString;\n        callback();\n      });\n    } else if (blobType === 'base64') {\n      blobOrBufferToBase64(att.data, function (b64) {\n        att.data = b64;\n        callback();\n      });\n    } else {\n      callback();\n    }\n  });\n}\n\nfunction preprocessAttachment(att, blobType, callback) {\n  if (att.stub) {\n    return callback();\n  }\n\n  if (typeof att.data === 'string') {\n    // input is a base64 string\n    preprocessString(att, blobType, callback);\n  } else {\n    // input is a blob\n    preprocessBlob(att, blobType, callback);\n  }\n}\n\nfunction preprocessAttachments(docInfos, blobType, callback) {\n  if (!docInfos.length) {\n    return callback();\n  }\n\n  var docv = 0;\n  var overallErr;\n  docInfos.forEach(function (docInfo) {\n    var attachments = docInfo.data && docInfo.data._attachments ? Object.keys(docInfo.data._attachments) : [];\n    var recv = 0;\n\n    if (!attachments.length) {\n      return done();\n    }\n\n    function processedAttachment(err) {\n      overallErr = err;\n      recv++;\n\n      if (recv === attachments.length) {\n        done();\n      }\n    }\n\n    for (var key in docInfo.data._attachments) {\n      if (docInfo.data._attachments.hasOwnProperty(key)) {\n        preprocessAttachment(docInfo.data._attachments[key], blobType, processedAttachment);\n      }\n    }\n  });\n\n  function done() {\n    docv++;\n\n    if (docInfos.length === docv) {\n      if (overallErr) {\n        callback(overallErr);\n      } else {\n        callback();\n      }\n    }\n  }\n}\n\nfunction updateDoc(revLimit, prev, docInfo, results, i, cb, writeDoc, newEdits) {\n  if (revExists(prev.rev_tree, docInfo.metadata.rev) && !newEdits) {\n    results[i] = docInfo;\n    return cb();\n  } // sometimes this is pre-calculated. historically not always\n\n\n  var previousWinningRev = prev.winningRev || winningRev(prev);\n  var previouslyDeleted = 'deleted' in prev ? prev.deleted : isDeleted(prev, previousWinningRev);\n  var deleted = 'deleted' in docInfo.metadata ? docInfo.metadata.deleted : isDeleted(docInfo.metadata);\n  var isRoot = /^1-/.test(docInfo.metadata.rev);\n\n  if (previouslyDeleted && !deleted && newEdits && isRoot) {\n    var newDoc = docInfo.data;\n    newDoc._rev = previousWinningRev;\n    newDoc._id = docInfo.metadata.id;\n    docInfo = parseDoc(newDoc, newEdits);\n  }\n\n  var merged = merge(prev.rev_tree, docInfo.metadata.rev_tree[0], revLimit);\n  var inConflict = newEdits && (previouslyDeleted && deleted && merged.conflicts !== 'new_leaf' || !previouslyDeleted && merged.conflicts !== 'new_leaf' || previouslyDeleted && !deleted && merged.conflicts === 'new_branch');\n\n  if (inConflict) {\n    var err = createError(REV_CONFLICT);\n    results[i] = err;\n    return cb();\n  }\n\n  var newRev = docInfo.metadata.rev;\n  docInfo.metadata.rev_tree = merged.tree;\n  docInfo.stemmedRevs = merged.stemmedRevs || [];\n  /* istanbul ignore else */\n\n  if (prev.rev_map) {\n    docInfo.metadata.rev_map = prev.rev_map; // used only by leveldb\n  } // recalculate\n\n\n  var winningRev$$1 = winningRev(docInfo.metadata);\n  var winningRevIsDeleted = isDeleted(docInfo.metadata, winningRev$$1); // calculate the total number of documents that were added/removed,\n  // from the perspective of total_rows/doc_count\n\n  var delta = previouslyDeleted === winningRevIsDeleted ? 0 : previouslyDeleted < winningRevIsDeleted ? -1 : 1;\n  var newRevIsDeleted;\n\n  if (newRev === winningRev$$1) {\n    // if the new rev is the same as the winning rev, we can reuse that value\n    newRevIsDeleted = winningRevIsDeleted;\n  } else {\n    // if they're not the same, then we need to recalculate\n    newRevIsDeleted = isDeleted(docInfo.metadata, newRev);\n  }\n\n  writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted, true, delta, i, cb);\n}\n\nfunction rootIsMissing(docInfo) {\n  return docInfo.metadata.rev_tree[0].ids[1].status === 'missing';\n}\n\nfunction processDocs(revLimit, docInfos, api, fetchedDocs, tx, results, writeDoc, opts, overallCallback) {\n  // Default to 1000 locally\n  revLimit = revLimit || 1000;\n\n  function insertDoc(docInfo, resultsIdx, callback) {\n    // Cant insert new deleted documents\n    var winningRev$$1 = winningRev(docInfo.metadata);\n    var deleted = isDeleted(docInfo.metadata, winningRev$$1);\n\n    if ('was_delete' in opts && deleted) {\n      results[resultsIdx] = createError(MISSING_DOC, 'deleted');\n      return callback();\n    } // 4712 - detect whether a new document was inserted with a _rev\n\n\n    var inConflict = newEdits && rootIsMissing(docInfo);\n\n    if (inConflict) {\n      var err = createError(REV_CONFLICT);\n      results[resultsIdx] = err;\n      return callback();\n    }\n\n    var delta = deleted ? 0 : 1;\n    writeDoc(docInfo, winningRev$$1, deleted, deleted, false, delta, resultsIdx, callback);\n  }\n\n  var newEdits = opts.new_edits;\n  var idsToDocs = new Map();\n  var docsDone = 0;\n  var docsToDo = docInfos.length;\n\n  function checkAllDocsDone() {\n    if (++docsDone === docsToDo && overallCallback) {\n      overallCallback();\n    }\n  }\n\n  docInfos.forEach(function (currentDoc, resultsIdx) {\n    if (currentDoc._id && isLocalId(currentDoc._id)) {\n      var fun = currentDoc._deleted ? '_removeLocal' : '_putLocal';\n      api[fun](currentDoc, {\n        ctx: tx\n      }, function (err, res) {\n        results[resultsIdx] = err || res;\n        checkAllDocsDone();\n      });\n      return;\n    }\n\n    var id = currentDoc.metadata.id;\n\n    if (idsToDocs.has(id)) {\n      docsToDo--; // duplicate\n\n      idsToDocs.get(id).push([currentDoc, resultsIdx]);\n    } else {\n      idsToDocs.set(id, [[currentDoc, resultsIdx]]);\n    }\n  }); // in the case of new_edits, the user can provide multiple docs\n  // with the same id. these need to be processed sequentially\n\n  idsToDocs.forEach(function (docs, id) {\n    var numDone = 0;\n\n    function docWritten() {\n      if (++numDone < docs.length) {\n        nextDoc();\n      } else {\n        checkAllDocsDone();\n      }\n    }\n\n    function nextDoc() {\n      var value = docs[numDone];\n      var currentDoc = value[0];\n      var resultsIdx = value[1];\n\n      if (fetchedDocs.has(id)) {\n        updateDoc(revLimit, fetchedDocs.get(id), currentDoc, results, resultsIdx, docWritten, writeDoc, newEdits);\n      } else {\n        // Ensure stemming applies to new writes as well\n        var merged = merge([], currentDoc.metadata.rev_tree[0], revLimit);\n        currentDoc.metadata.rev_tree = merged.tree;\n        currentDoc.stemmedRevs = merged.stemmedRevs || [];\n        insertDoc(currentDoc, resultsIdx, docWritten);\n      }\n    }\n\n    nextDoc();\n  });\n}\n\nexport { allDocsKeysQuery, parseDoc, preprocessAttachments, processDocs, updateDoc };","map":null,"metadata":{},"sourceType":"module"}