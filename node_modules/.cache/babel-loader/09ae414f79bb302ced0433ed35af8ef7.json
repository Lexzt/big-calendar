{"ast":null,"code":"import { assign, pick, changesHandler, nextTick, clone, filterChange, uuid, guardedConsole, toPromise, hasLocalStorage } from 'pouchdb-utils';\nimport { createError, IDB_ERROR, MISSING_STUB, MISSING_DOC, REV_CONFLICT } from 'pouchdb-errors';\nimport { safeJsonParse, safeJsonStringify } from 'pouchdb-json';\nimport { btoa, readAsBinaryString, base64StringToBlobOrBuffer, blob } from 'pouchdb-binary-utils';\nimport { Map, Set } from 'pouchdb-collections';\nimport { preprocessAttachments, processDocs, isLocalId, parseDoc } from 'pouchdb-adapter-utils';\nimport { compactTree, collectConflicts, isDeleted, isLocalId as isLocalId$1, traverseRevTree, winningRev, latest } from 'pouchdb-merge'; // IndexedDB requires a versioned database structure, so we use the\n// version here to manage migrations.\n\nvar ADAPTER_VERSION = 5; // The object stores created for each database\n// DOC_STORE stores the document meta data, its revision history and state\n// Keyed by document id\n\nvar DOC_STORE = 'document-store'; // BY_SEQ_STORE stores a particular version of a document, keyed by its\n// sequence id\n\nvar BY_SEQ_STORE = 'by-sequence'; // Where we store attachments\n\nvar ATTACH_STORE = 'attach-store'; // Where we store many-to-many relations\n// between attachment digests and seqs\n\nvar ATTACH_AND_SEQ_STORE = 'attach-seq-store'; // Where we store database-wide meta data in a single record\n// keyed by id: META_STORE\n\nvar META_STORE = 'meta-store'; // Where we store local documents\n\nvar LOCAL_STORE = 'local-store'; // Where we detect blob support\n\nvar DETECT_BLOB_SUPPORT_STORE = 'detect-blob-support';\n\nfunction idbError(callback) {\n  return function (evt) {\n    var message = 'unknown_error';\n\n    if (evt.target && evt.target.error) {\n      message = evt.target.error.name || evt.target.error.message;\n    }\n\n    callback(createError(IDB_ERROR, message, evt.type));\n  };\n} // Unfortunately, the metadata has to be stringified\n// when it is put into the database, because otherwise\n// IndexedDB can throw errors for deeply-nested objects.\n// Originally we just used JSON.parse/JSON.stringify; now\n// we use this custom vuvuzela library that avoids recursion.\n// If we could do it all over again, we'd probably use a\n// format for the revision trees other than JSON.\n\n\nfunction encodeMetadata(metadata, winningRev$$1, deleted) {\n  return {\n    data: safeJsonStringify(metadata),\n    winningRev: winningRev$$1,\n    deletedOrLocal: deleted ? '1' : '0',\n    seq: metadata.seq,\n    // highest seq for this doc\n    id: metadata.id\n  };\n}\n\nfunction decodeMetadata(storedObject) {\n  if (!storedObject) {\n    return null;\n  }\n\n  var metadata = safeJsonParse(storedObject.data);\n  metadata.winningRev = storedObject.winningRev;\n  metadata.deleted = storedObject.deletedOrLocal === '1';\n  metadata.seq = storedObject.seq;\n  return metadata;\n} // read the doc back out from the database. we don't store the\n// _id or _rev because we already have _doc_id_rev.\n\n\nfunction decodeDoc(doc) {\n  if (!doc) {\n    return doc;\n  }\n\n  var idx = doc._doc_id_rev.lastIndexOf(':');\n\n  doc._id = doc._doc_id_rev.substring(0, idx - 1);\n  doc._rev = doc._doc_id_rev.substring(idx + 1);\n  delete doc._doc_id_rev;\n  return doc;\n} // Read a blob from the database, encoding as necessary\n// and translating from base64 if the IDB doesn't support\n// native Blobs\n\n\nfunction readBlobData(body, type, asBlob, callback) {\n  if (asBlob) {\n    if (!body) {\n      callback(blob([''], {\n        type: type\n      }));\n    } else if (typeof body !== 'string') {\n      // we have blob support\n      callback(body);\n    } else {\n      // no blob support\n      callback(base64StringToBlobOrBuffer(body, type));\n    }\n  } else {\n    // as base64 string\n    if (!body) {\n      callback('');\n    } else if (typeof body !== 'string') {\n      // we have blob support\n      readAsBinaryString(body, function (binary) {\n        callback(btoa(binary));\n      });\n    } else {\n      // no blob support\n      callback(body);\n    }\n  }\n}\n\nfunction fetchAttachmentsIfNecessary(doc, opts, txn, cb) {\n  var attachments = Object.keys(doc._attachments || {});\n\n  if (!attachments.length) {\n    return cb && cb();\n  }\n\n  var numDone = 0;\n\n  function checkDone() {\n    if (++numDone === attachments.length && cb) {\n      cb();\n    }\n  }\n\n  function fetchAttachment(doc, att) {\n    var attObj = doc._attachments[att];\n    var digest = attObj.digest;\n    var req = txn.objectStore(ATTACH_STORE).get(digest);\n\n    req.onsuccess = function (e) {\n      attObj.body = e.target.result.body;\n      checkDone();\n    };\n  }\n\n  attachments.forEach(function (att) {\n    if (opts.attachments && opts.include_docs) {\n      fetchAttachment(doc, att);\n    } else {\n      doc._attachments[att].stub = true;\n      checkDone();\n    }\n  });\n} // IDB-specific postprocessing necessary because\n// we don't know whether we stored a true Blob or\n// a base64-encoded string, and if it's a Blob it\n// needs to be read outside of the transaction context\n\n\nfunction postProcessAttachments(results, asBlob) {\n  return Promise.all(results.map(function (row) {\n    if (row.doc && row.doc._attachments) {\n      var attNames = Object.keys(row.doc._attachments);\n      return Promise.all(attNames.map(function (att) {\n        var attObj = row.doc._attachments[att];\n\n        if (!('body' in attObj)) {\n          // already processed\n          return;\n        }\n\n        var body = attObj.body;\n        var type = attObj.content_type;\n        return new Promise(function (resolve) {\n          readBlobData(body, type, asBlob, function (data) {\n            row.doc._attachments[att] = assign(pick(attObj, ['digest', 'content_type']), {\n              data: data\n            });\n            resolve();\n          });\n        });\n      }));\n    }\n  }));\n}\n\nfunction compactRevs(revs, docId, txn) {\n  var possiblyOrphanedDigests = [];\n  var seqStore = txn.objectStore(BY_SEQ_STORE);\n  var attStore = txn.objectStore(ATTACH_STORE);\n  var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n  var count = revs.length;\n\n  function checkDone() {\n    count--;\n\n    if (!count) {\n      // done processing all revs\n      deleteOrphanedAttachments();\n    }\n  }\n\n  function deleteOrphanedAttachments() {\n    if (!possiblyOrphanedDigests.length) {\n      return;\n    }\n\n    possiblyOrphanedDigests.forEach(function (digest) {\n      var countReq = attAndSeqStore.index('digestSeq').count(IDBKeyRange.bound(digest + '::', digest + \"::\\uFFFF\", false, false));\n\n      countReq.onsuccess = function (e) {\n        var count = e.target.result;\n\n        if (!count) {\n          // orphaned\n          attStore.delete(digest);\n        }\n      };\n    });\n  }\n\n  revs.forEach(function (rev) {\n    var index = seqStore.index('_doc_id_rev');\n    var key = docId + \"::\" + rev;\n\n    index.getKey(key).onsuccess = function (e) {\n      var seq = e.target.result;\n\n      if (typeof seq !== 'number') {\n        return checkDone();\n      }\n\n      seqStore.delete(seq);\n      var cursor = attAndSeqStore.index('seq').openCursor(IDBKeyRange.only(seq));\n\n      cursor.onsuccess = function (event) {\n        var cursor = event.target.result;\n\n        if (cursor) {\n          var digest = cursor.value.digestSeq.split('::')[0];\n          possiblyOrphanedDigests.push(digest);\n          attAndSeqStore.delete(cursor.primaryKey);\n          cursor.continue();\n        } else {\n          // done\n          checkDone();\n        }\n      };\n    };\n  });\n}\n\nfunction openTransactionSafely(idb, stores, mode) {\n  try {\n    return {\n      txn: idb.transaction(stores, mode)\n    };\n  } catch (err) {\n    return {\n      error: err\n    };\n  }\n}\n\nvar changesHandler$1 = new changesHandler();\n\nfunction idbBulkDocs(dbOpts, req, opts, api, idb, callback) {\n  var docInfos = req.docs;\n  var txn;\n  var docStore;\n  var bySeqStore;\n  var attachStore;\n  var attachAndSeqStore;\n  var metaStore;\n  var docInfoError;\n  var metaDoc;\n\n  for (var i = 0, len = docInfos.length; i < len; i++) {\n    var doc = docInfos[i];\n\n    if (doc._id && isLocalId(doc._id)) {\n      continue;\n    }\n\n    doc = docInfos[i] = parseDoc(doc, opts.new_edits, dbOpts);\n\n    if (doc.error && !docInfoError) {\n      docInfoError = doc;\n    }\n  }\n\n  if (docInfoError) {\n    return callback(docInfoError);\n  }\n\n  var allDocsProcessed = false;\n  var docCountDelta = 0;\n  var results = new Array(docInfos.length);\n  var fetchedDocs = new Map();\n  var preconditionErrored = false;\n  var blobType = api._meta.blobSupport ? 'blob' : 'base64';\n  preprocessAttachments(docInfos, blobType, function (err) {\n    if (err) {\n      return callback(err);\n    }\n\n    startTransaction();\n  });\n\n  function startTransaction() {\n    var stores = [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE, LOCAL_STORE, ATTACH_AND_SEQ_STORE, META_STORE];\n    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n\n    txn = txnResult.txn;\n    txn.onabort = idbError(callback);\n    txn.ontimeout = idbError(callback);\n    txn.oncomplete = complete;\n    docStore = txn.objectStore(DOC_STORE);\n    bySeqStore = txn.objectStore(BY_SEQ_STORE);\n    attachStore = txn.objectStore(ATTACH_STORE);\n    attachAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n    metaStore = txn.objectStore(META_STORE);\n\n    metaStore.get(META_STORE).onsuccess = function (e) {\n      metaDoc = e.target.result;\n      updateDocCountIfReady();\n    };\n\n    verifyAttachments(function (err) {\n      if (err) {\n        preconditionErrored = true;\n        return callback(err);\n      }\n\n      fetchExistingDocs();\n    });\n  }\n\n  function onAllDocsProcessed() {\n    allDocsProcessed = true;\n    updateDocCountIfReady();\n  }\n\n  function idbProcessDocs() {\n    processDocs(dbOpts.revs_limit, docInfos, api, fetchedDocs, txn, results, writeDoc, opts, onAllDocsProcessed);\n  }\n\n  function updateDocCountIfReady() {\n    if (!metaDoc || !allDocsProcessed) {\n      return;\n    } // caching the docCount saves a lot of time in allDocs() and\n    // info(), which is why we go to all the trouble of doing this\n\n\n    metaDoc.docCount += docCountDelta;\n    metaStore.put(metaDoc);\n  }\n\n  function fetchExistingDocs() {\n    if (!docInfos.length) {\n      return;\n    }\n\n    var numFetched = 0;\n\n    function checkDone() {\n      if (++numFetched === docInfos.length) {\n        idbProcessDocs();\n      }\n    }\n\n    function readMetadata(event) {\n      var metadata = decodeMetadata(event.target.result);\n\n      if (metadata) {\n        fetchedDocs.set(metadata.id, metadata);\n      }\n\n      checkDone();\n    }\n\n    for (var i = 0, len = docInfos.length; i < len; i++) {\n      var docInfo = docInfos[i];\n\n      if (docInfo._id && isLocalId(docInfo._id)) {\n        checkDone(); // skip local docs\n\n        continue;\n      }\n\n      var req = docStore.get(docInfo.metadata.id);\n      req.onsuccess = readMetadata;\n    }\n  }\n\n  function complete() {\n    if (preconditionErrored) {\n      return;\n    }\n\n    changesHandler$1.notify(api._meta.name);\n    callback(null, results);\n  }\n\n  function verifyAttachment(digest, callback) {\n    var req = attachStore.get(digest);\n\n    req.onsuccess = function (e) {\n      if (!e.target.result) {\n        var err = createError(MISSING_STUB, 'unknown stub attachment with digest ' + digest);\n        err.status = 412;\n        callback(err);\n      } else {\n        callback();\n      }\n    };\n  }\n\n  function verifyAttachments(finish) {\n    var digests = [];\n    docInfos.forEach(function (docInfo) {\n      if (docInfo.data && docInfo.data._attachments) {\n        Object.keys(docInfo.data._attachments).forEach(function (filename) {\n          var att = docInfo.data._attachments[filename];\n\n          if (att.stub) {\n            digests.push(att.digest);\n          }\n        });\n      }\n    });\n\n    if (!digests.length) {\n      return finish();\n    }\n\n    var numDone = 0;\n    var err;\n\n    function checkDone() {\n      if (++numDone === digests.length) {\n        finish(err);\n      }\n    }\n\n    digests.forEach(function (digest) {\n      verifyAttachment(digest, function (attErr) {\n        if (attErr && !err) {\n          err = attErr;\n        }\n\n        checkDone();\n      });\n    });\n  }\n\n  function writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted, isUpdate, delta, resultsIdx, callback) {\n    docInfo.metadata.winningRev = winningRev$$1;\n    docInfo.metadata.deleted = winningRevIsDeleted;\n    var doc = docInfo.data;\n    doc._id = docInfo.metadata.id;\n    doc._rev = docInfo.metadata.rev;\n\n    if (newRevIsDeleted) {\n      doc._deleted = true;\n    }\n\n    var hasAttachments = doc._attachments && Object.keys(doc._attachments).length;\n\n    if (hasAttachments) {\n      return writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted, isUpdate, resultsIdx, callback);\n    }\n\n    docCountDelta += delta;\n    updateDocCountIfReady();\n    finishDoc(docInfo, winningRev$$1, winningRevIsDeleted, isUpdate, resultsIdx, callback);\n  }\n\n  function finishDoc(docInfo, winningRev$$1, winningRevIsDeleted, isUpdate, resultsIdx, callback) {\n    var doc = docInfo.data;\n    var metadata = docInfo.metadata;\n    doc._doc_id_rev = metadata.id + '::' + metadata.rev;\n    delete doc._id;\n    delete doc._rev;\n\n    function afterPutDoc(e) {\n      var revsToDelete = docInfo.stemmedRevs || [];\n\n      if (isUpdate && api.auto_compaction) {\n        revsToDelete = revsToDelete.concat(compactTree(docInfo.metadata));\n      }\n\n      if (revsToDelete && revsToDelete.length) {\n        compactRevs(revsToDelete, docInfo.metadata.id, txn);\n      }\n\n      metadata.seq = e.target.result; // Current _rev is calculated from _rev_tree on read\n      // delete metadata.rev;\n\n      var metadataToStore = encodeMetadata(metadata, winningRev$$1, winningRevIsDeleted);\n      var metaDataReq = docStore.put(metadataToStore);\n      metaDataReq.onsuccess = afterPutMetadata;\n    }\n\n    function afterPutDocError(e) {\n      // ConstraintError, need to update, not put (see #1638 for details)\n      e.preventDefault(); // avoid transaction abort\n\n      e.stopPropagation(); // avoid transaction onerror\n\n      var index = bySeqStore.index('_doc_id_rev');\n      var getKeyReq = index.getKey(doc._doc_id_rev);\n\n      getKeyReq.onsuccess = function (e) {\n        var putReq = bySeqStore.put(doc, e.target.result);\n        putReq.onsuccess = afterPutDoc;\n      };\n    }\n\n    function afterPutMetadata() {\n      results[resultsIdx] = {\n        ok: true,\n        id: metadata.id,\n        rev: metadata.rev\n      };\n      fetchedDocs.set(docInfo.metadata.id, docInfo.metadata);\n      insertAttachmentMappings(docInfo, metadata.seq, callback);\n    }\n\n    var putReq = bySeqStore.put(doc);\n    putReq.onsuccess = afterPutDoc;\n    putReq.onerror = afterPutDocError;\n  }\n\n  function writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted, isUpdate, resultsIdx, callback) {\n    var doc = docInfo.data;\n    var numDone = 0;\n    var attachments = Object.keys(doc._attachments);\n\n    function collectResults() {\n      if (numDone === attachments.length) {\n        finishDoc(docInfo, winningRev$$1, winningRevIsDeleted, isUpdate, resultsIdx, callback);\n      }\n    }\n\n    function attachmentSaved() {\n      numDone++;\n      collectResults();\n    }\n\n    attachments.forEach(function (key) {\n      var att = docInfo.data._attachments[key];\n\n      if (!att.stub) {\n        var data = att.data;\n        delete att.data;\n        att.revpos = parseInt(winningRev$$1, 10);\n        var digest = att.digest;\n        saveAttachment(digest, data, attachmentSaved);\n      } else {\n        numDone++;\n        collectResults();\n      }\n    });\n  } // map seqs to attachment digests, which\n  // we will need later during compaction\n\n\n  function insertAttachmentMappings(docInfo, seq, callback) {\n    var attsAdded = 0;\n    var attsToAdd = Object.keys(docInfo.data._attachments || {});\n\n    if (!attsToAdd.length) {\n      return callback();\n    }\n\n    function checkDone() {\n      if (++attsAdded === attsToAdd.length) {\n        callback();\n      }\n    }\n\n    function add(att) {\n      var digest = docInfo.data._attachments[att].digest;\n      var req = attachAndSeqStore.put({\n        seq: seq,\n        digestSeq: digest + '::' + seq\n      });\n      req.onsuccess = checkDone;\n\n      req.onerror = function (e) {\n        // this callback is for a constaint error, which we ignore\n        // because this docid/rev has already been associated with\n        // the digest (e.g. when new_edits == false)\n        e.preventDefault(); // avoid transaction abort\n\n        e.stopPropagation(); // avoid transaction onerror\n\n        checkDone();\n      };\n    }\n\n    for (var i = 0; i < attsToAdd.length; i++) {\n      add(attsToAdd[i]); // do in parallel\n    }\n  }\n\n  function saveAttachment(digest, data, callback) {\n    var getKeyReq = attachStore.count(digest);\n\n    getKeyReq.onsuccess = function (e) {\n      var count = e.target.result;\n\n      if (count) {\n        return callback(); // already exists\n      }\n\n      var newAtt = {\n        digest: digest,\n        body: data\n      };\n      var putReq = attachStore.put(newAtt);\n      putReq.onsuccess = callback;\n    };\n  }\n} // Abstraction over IDBCursor and getAll()/getAllKeys() that allows us to batch our operations\n// while falling back to a normal IDBCursor operation on browsers that don't support getAll() or\n// getAllKeys(). This allows for a much faster implementation than just straight-up cursors, because\n// we're not processing each document one-at-a-time.\n\n\nfunction runBatchedCursor(objectStore, keyRange, descending, batchSize, onBatch) {\n  if (batchSize === -1) {\n    batchSize = 1000;\n  } // Bail out of getAll()/getAllKeys() in the following cases:\n  // 1) either method is unsupported - we need both\n  // 2) batchSize is 1 (might as well use IDBCursor)\n  // 3) descending – no real way to do this via getAll()/getAllKeys()\n\n\n  var useGetAll = typeof objectStore.getAll === 'function' && typeof objectStore.getAllKeys === 'function' && batchSize > 1 && !descending;\n  var keysBatch;\n  var valuesBatch;\n  var pseudoCursor;\n\n  function onGetAll(e) {\n    valuesBatch = e.target.result;\n\n    if (keysBatch) {\n      onBatch(keysBatch, valuesBatch, pseudoCursor);\n    }\n  }\n\n  function onGetAllKeys(e) {\n    keysBatch = e.target.result;\n\n    if (valuesBatch) {\n      onBatch(keysBatch, valuesBatch, pseudoCursor);\n    }\n  }\n\n  function continuePseudoCursor() {\n    if (!keysBatch.length) {\n      // no more results\n      return onBatch();\n    } // fetch next batch, exclusive start\n\n\n    var lastKey = keysBatch[keysBatch.length - 1];\n    var newKeyRange;\n\n    if (keyRange && keyRange.upper) {\n      try {\n        newKeyRange = IDBKeyRange.bound(lastKey, keyRange.upper, true, keyRange.upperOpen);\n      } catch (e) {\n        if (e.name === \"DataError\" && e.code === 0) {\n          return onBatch(); // we're done, startkey and endkey are equal\n        }\n      }\n    } else {\n      newKeyRange = IDBKeyRange.lowerBound(lastKey, true);\n    }\n\n    keyRange = newKeyRange;\n    keysBatch = null;\n    valuesBatch = null;\n    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n  }\n\n  function onCursor(e) {\n    var cursor = e.target.result;\n\n    if (!cursor) {\n      // done\n      return onBatch();\n    } // regular IDBCursor acts like a batch where batch size is always 1\n\n\n    onBatch([cursor.key], [cursor.value], cursor);\n  }\n\n  if (useGetAll) {\n    pseudoCursor = {\n      \"continue\": continuePseudoCursor\n    };\n    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n  } else if (descending) {\n    objectStore.openCursor(keyRange, 'prev').onsuccess = onCursor;\n  } else {\n    objectStore.openCursor(keyRange).onsuccess = onCursor;\n  }\n} // simple shim for objectStore.getAll(), falling back to IDBCursor\n\n\nfunction getAll(objectStore, keyRange, onSuccess) {\n  if (typeof objectStore.getAll === 'function') {\n    // use native getAll\n    objectStore.getAll(keyRange).onsuccess = onSuccess;\n    return;\n  } // fall back to cursors\n\n\n  var values = [];\n\n  function onCursor(e) {\n    var cursor = e.target.result;\n\n    if (cursor) {\n      values.push(cursor.value);\n      cursor.continue();\n    } else {\n      onSuccess({\n        target: {\n          result: values\n        }\n      });\n    }\n  }\n\n  objectStore.openCursor(keyRange).onsuccess = onCursor;\n}\n\nfunction allDocsKeys(keys, docStore, onBatch) {\n  // It's not guaranted to be returned in right order  \n  var valuesBatch = new Array(keys.length);\n  var count = 0;\n  keys.forEach(function (key, index) {\n    docStore.get(key).onsuccess = function (event) {\n      if (event.target.result) {\n        valuesBatch[index] = event.target.result;\n      } else {\n        valuesBatch[index] = {\n          key: key,\n          error: 'not_found'\n        };\n      }\n\n      count++;\n\n      if (count === keys.length) {\n        onBatch(keys, valuesBatch, {});\n      }\n    };\n  });\n}\n\nfunction createKeyRange(start, end, inclusiveEnd, key, descending) {\n  try {\n    if (start && end) {\n      if (descending) {\n        return IDBKeyRange.bound(end, start, !inclusiveEnd, false);\n      } else {\n        return IDBKeyRange.bound(start, end, false, !inclusiveEnd);\n      }\n    } else if (start) {\n      if (descending) {\n        return IDBKeyRange.upperBound(start);\n      } else {\n        return IDBKeyRange.lowerBound(start);\n      }\n    } else if (end) {\n      if (descending) {\n        return IDBKeyRange.lowerBound(end, !inclusiveEnd);\n      } else {\n        return IDBKeyRange.upperBound(end, !inclusiveEnd);\n      }\n    } else if (key) {\n      return IDBKeyRange.only(key);\n    }\n  } catch (e) {\n    return {\n      error: e\n    };\n  }\n\n  return null;\n}\n\nfunction idbAllDocs(opts, idb, callback) {\n  var start = 'startkey' in opts ? opts.startkey : false;\n  var end = 'endkey' in opts ? opts.endkey : false;\n  var key = 'key' in opts ? opts.key : false;\n  var keys = 'keys' in opts ? opts.keys : false;\n  var skip = opts.skip || 0;\n  var limit = typeof opts.limit === 'number' ? opts.limit : -1;\n  var inclusiveEnd = opts.inclusive_end !== false;\n  var keyRange;\n  var keyRangeError;\n\n  if (!keys) {\n    keyRange = createKeyRange(start, end, inclusiveEnd, key, opts.descending);\n    keyRangeError = keyRange && keyRange.error;\n\n    if (keyRangeError && !(keyRangeError.name === \"DataError\" && keyRangeError.code === 0)) {\n      // DataError with error code 0 indicates start is less than end, so\n      // can just do an empty query. Else need to throw\n      return callback(createError(IDB_ERROR, keyRangeError.name, keyRangeError.message));\n    }\n  }\n\n  var stores = [DOC_STORE, BY_SEQ_STORE, META_STORE];\n\n  if (opts.attachments) {\n    stores.push(ATTACH_STORE);\n  }\n\n  var txnResult = openTransactionSafely(idb, stores, 'readonly');\n\n  if (txnResult.error) {\n    return callback(txnResult.error);\n  }\n\n  var txn = txnResult.txn;\n  txn.oncomplete = onTxnComplete;\n  txn.onabort = idbError(callback);\n  var docStore = txn.objectStore(DOC_STORE);\n  var seqStore = txn.objectStore(BY_SEQ_STORE);\n  var metaStore = txn.objectStore(META_STORE);\n  var docIdRevIndex = seqStore.index('_doc_id_rev');\n  var results = [];\n  var docCount;\n  var updateSeq;\n\n  metaStore.get(META_STORE).onsuccess = function (e) {\n    docCount = e.target.result.docCount;\n  };\n  /* istanbul ignore if */\n\n\n  if (opts.update_seq) {\n    getMaxUpdateSeq(seqStore, function (e) {\n      if (e.target.result && e.target.result.length > 0) {\n        updateSeq = e.target.result[0];\n      }\n    });\n  }\n\n  function getMaxUpdateSeq(objectStore, onSuccess) {\n    function onCursor(e) {\n      var cursor = e.target.result;\n      var maxKey = undefined;\n\n      if (cursor && cursor.key) {\n        maxKey = cursor.key;\n      }\n\n      return onSuccess({\n        target: {\n          result: [maxKey]\n        }\n      });\n    }\n\n    objectStore.openCursor(null, 'prev').onsuccess = onCursor;\n  } // if the user specifies include_docs=true, then we don't\n  // want to block the main cursor while we're fetching the doc\n\n\n  function fetchDocAsynchronously(metadata, row, winningRev$$1) {\n    var key = metadata.id + \"::\" + winningRev$$1;\n\n    docIdRevIndex.get(key).onsuccess = function onGetDoc(e) {\n      row.doc = decodeDoc(e.target.result) || {};\n\n      if (opts.conflicts) {\n        var conflicts = collectConflicts(metadata);\n\n        if (conflicts.length) {\n          row.doc._conflicts = conflicts;\n        }\n      }\n\n      fetchAttachmentsIfNecessary(row.doc, opts, txn);\n    };\n  }\n\n  function allDocsInner(winningRev$$1, metadata) {\n    var row = {\n      id: metadata.id,\n      key: metadata.id,\n      value: {\n        rev: winningRev$$1\n      }\n    };\n    var deleted = metadata.deleted;\n\n    if (deleted) {\n      if (keys) {\n        results.push(row); // deleted docs are okay with \"keys\" requests\n\n        row.value.deleted = true;\n        row.doc = null;\n      }\n    } else if (skip-- <= 0) {\n      results.push(row);\n\n      if (opts.include_docs) {\n        fetchDocAsynchronously(metadata, row, winningRev$$1);\n      }\n    }\n  }\n\n  function processBatch(batchValues) {\n    for (var i = 0, len = batchValues.length; i < len; i++) {\n      if (results.length === limit) {\n        break;\n      }\n\n      var batchValue = batchValues[i];\n\n      if (batchValue.error && keys) {\n        // key was not found with \"keys\" requests\n        results.push(batchValue);\n        continue;\n      }\n\n      var metadata = decodeMetadata(batchValue);\n      var winningRev$$1 = metadata.winningRev;\n      allDocsInner(winningRev$$1, metadata);\n    }\n  }\n\n  function onBatch(batchKeys, batchValues, cursor) {\n    if (!cursor) {\n      return;\n    }\n\n    processBatch(batchValues);\n\n    if (results.length < limit) {\n      cursor.continue();\n    }\n  }\n\n  function onGetAll(e) {\n    var values = e.target.result;\n\n    if (opts.descending) {\n      values = values.reverse();\n    }\n\n    processBatch(values);\n  }\n\n  function onResultsReady() {\n    var returnVal = {\n      total_rows: docCount,\n      offset: opts.skip,\n      rows: results\n    };\n    /* istanbul ignore if */\n\n    if (opts.update_seq && updateSeq !== undefined) {\n      returnVal.update_seq = updateSeq;\n    }\n\n    callback(null, returnVal);\n  }\n\n  function onTxnComplete() {\n    if (opts.attachments) {\n      postProcessAttachments(results, opts.binary).then(onResultsReady);\n    } else {\n      onResultsReady();\n    }\n  } // don't bother doing any requests if start > end or limit === 0\n\n\n  if (keyRangeError || limit === 0) {\n    return;\n  }\n\n  if (keys) {\n    return allDocsKeys(opts.keys, docStore, onBatch);\n  }\n\n  if (limit === -1) {\n    // just fetch everything\n    return getAll(docStore, keyRange, onGetAll);\n  } // else do a cursor\n  // choose a batch size based on the skip, since we'll need to skip that many\n\n\n  runBatchedCursor(docStore, keyRange, opts.descending, limit + skip, onBatch);\n} //\n// Blobs are not supported in all versions of IndexedDB, notably\n// Chrome <37 and Android <5. In those versions, storing a blob will throw.\n//\n// Various other blob bugs exist in Chrome v37-42 (inclusive).\n// Detecting them is expensive and confusing to users, and Chrome 37-42\n// is at very low usage worldwide, so we do a hacky userAgent check instead.\n//\n// content-type bug: https://code.google.com/p/chromium/issues/detail?id=408120\n// 404 bug: https://code.google.com/p/chromium/issues/detail?id=447916\n// FileReader bug: https://code.google.com/p/chromium/issues/detail?id=447836\n//\n\n\nfunction checkBlobSupport(txn) {\n  return new Promise(function (resolve) {\n    var blob$$1 = blob(['']);\n    var req = txn.objectStore(DETECT_BLOB_SUPPORT_STORE).put(blob$$1, 'key');\n\n    req.onsuccess = function () {\n      var matchedChrome = navigator.userAgent.match(/Chrome\\/(\\d+)/);\n      var matchedEdge = navigator.userAgent.match(/Edge\\//); // MS Edge pretends to be Chrome 42:\n      // https://msdn.microsoft.com/en-us/library/hh869301%28v=vs.85%29.aspx\n\n      resolve(matchedEdge || !matchedChrome || parseInt(matchedChrome[1], 10) >= 43);\n    };\n\n    req.onerror = txn.onabort = function (e) {\n      // If the transaction aborts now its due to not being able to\n      // write to the database, likely due to the disk being full\n      e.preventDefault();\n      e.stopPropagation();\n      resolve(false);\n    };\n  }).catch(function () {\n    return false; // error, so assume unsupported\n  });\n}\n\nfunction countDocs(txn, cb) {\n  var index = txn.objectStore(DOC_STORE).index('deletedOrLocal');\n\n  index.count(IDBKeyRange.only('0')).onsuccess = function (e) {\n    cb(e.target.result);\n  };\n} // This task queue ensures that IDB open calls are done in their own tick\n\n\nvar running = false;\nvar queue = [];\n\nfunction tryCode(fun, err, res, PouchDB) {\n  try {\n    fun(err, res);\n  } catch (err) {\n    // Shouldn't happen, but in some odd cases\n    // IndexedDB implementations might throw a sync\n    // error, in which case this will at least log it.\n    PouchDB.emit('error', err);\n  }\n}\n\nfunction applyNext() {\n  if (running || !queue.length) {\n    return;\n  }\n\n  running = true;\n  queue.shift()();\n}\n\nfunction enqueueTask(action, callback, PouchDB) {\n  queue.push(function runAction() {\n    action(function runCallback(err, res) {\n      tryCode(callback, err, res, PouchDB);\n      running = false;\n      nextTick(function runNext() {\n        applyNext(PouchDB);\n      });\n    });\n  });\n  applyNext();\n}\n\nfunction changes(opts, api, dbName, idb) {\n  opts = clone(opts);\n\n  if (opts.continuous) {\n    var id = dbName + ':' + uuid();\n    changesHandler$1.addListener(dbName, id, api, opts);\n    changesHandler$1.notify(dbName);\n    return {\n      cancel: function cancel() {\n        changesHandler$1.removeListener(dbName, id);\n      }\n    };\n  }\n\n  var docIds = opts.doc_ids && new Set(opts.doc_ids);\n  opts.since = opts.since || 0;\n  var lastSeq = opts.since;\n  var limit = 'limit' in opts ? opts.limit : -1;\n\n  if (limit === 0) {\n    limit = 1; // per CouchDB _changes spec\n  }\n\n  var results = [];\n  var numResults = 0;\n  var filter = filterChange(opts);\n  var docIdsToMetadata = new Map();\n  var txn;\n  var bySeqStore;\n  var docStore;\n  var docIdRevIndex;\n\n  function onBatch(batchKeys, batchValues, cursor) {\n    if (!cursor || !batchKeys.length) {\n      // done\n      return;\n    }\n\n    var winningDocs = new Array(batchKeys.length);\n    var metadatas = new Array(batchKeys.length);\n\n    function processMetadataAndWinningDoc(metadata, winningDoc) {\n      var change = opts.processChange(winningDoc, metadata, opts);\n      lastSeq = change.seq = metadata.seq;\n      var filtered = filter(change);\n\n      if (typeof filtered === 'object') {\n        // anything but true/false indicates error\n        return Promise.reject(filtered);\n      }\n\n      if (!filtered) {\n        return Promise.resolve();\n      }\n\n      numResults++;\n\n      if (opts.return_docs) {\n        results.push(change);\n      } // process the attachment immediately\n      // for the benefit of live listeners\n\n\n      if (opts.attachments && opts.include_docs) {\n        return new Promise(function (resolve) {\n          fetchAttachmentsIfNecessary(winningDoc, opts, txn, function () {\n            postProcessAttachments([change], opts.binary).then(function () {\n              resolve(change);\n            });\n          });\n        });\n      } else {\n        return Promise.resolve(change);\n      }\n    }\n\n    function onBatchDone() {\n      var promises = [];\n\n      for (var i = 0, len = winningDocs.length; i < len; i++) {\n        if (numResults === limit) {\n          break;\n        }\n\n        var winningDoc = winningDocs[i];\n\n        if (!winningDoc) {\n          continue;\n        }\n\n        var metadata = metadatas[i];\n        promises.push(processMetadataAndWinningDoc(metadata, winningDoc));\n      }\n\n      Promise.all(promises).then(function (changes) {\n        for (var i = 0, len = changes.length; i < len; i++) {\n          if (changes[i]) {\n            opts.onChange(changes[i]);\n          }\n        }\n      }).catch(opts.complete);\n\n      if (numResults !== limit) {\n        cursor.continue();\n      }\n    } // Fetch all metadatas/winningdocs from this batch in parallel, then process\n    // them all only once all data has been collected. This is done in parallel\n    // because it's faster than doing it one-at-a-time.\n\n\n    var numDone = 0;\n    batchValues.forEach(function (value, i) {\n      var doc = decodeDoc(value);\n      var seq = batchKeys[i];\n      fetchWinningDocAndMetadata(doc, seq, function (metadata, winningDoc) {\n        metadatas[i] = metadata;\n        winningDocs[i] = winningDoc;\n\n        if (++numDone === batchKeys.length) {\n          onBatchDone();\n        }\n      });\n    });\n  }\n\n  function onGetMetadata(doc, seq, metadata, cb) {\n    if (metadata.seq !== seq) {\n      // some other seq is later\n      return cb();\n    }\n\n    if (metadata.winningRev === doc._rev) {\n      // this is the winning doc\n      return cb(metadata, doc);\n    } // fetch winning doc in separate request\n\n\n    var docIdRev = doc._id + '::' + metadata.winningRev;\n    var req = docIdRevIndex.get(docIdRev);\n\n    req.onsuccess = function (e) {\n      cb(metadata, decodeDoc(e.target.result));\n    };\n  }\n\n  function fetchWinningDocAndMetadata(doc, seq, cb) {\n    if (docIds && !docIds.has(doc._id)) {\n      return cb();\n    }\n\n    var metadata = docIdsToMetadata.get(doc._id);\n\n    if (metadata) {\n      // cached\n      return onGetMetadata(doc, seq, metadata, cb);\n    } // metadata not cached, have to go fetch it\n\n\n    docStore.get(doc._id).onsuccess = function (e) {\n      metadata = decodeMetadata(e.target.result);\n      docIdsToMetadata.set(doc._id, metadata);\n      onGetMetadata(doc, seq, metadata, cb);\n    };\n  }\n\n  function finish() {\n    opts.complete(null, {\n      results: results,\n      last_seq: lastSeq\n    });\n  }\n\n  function onTxnComplete() {\n    if (!opts.continuous && opts.attachments) {\n      // cannot guarantee that postProcessing was already done,\n      // so do it again\n      postProcessAttachments(results).then(finish);\n    } else {\n      finish();\n    }\n  }\n\n  var objectStores = [DOC_STORE, BY_SEQ_STORE];\n\n  if (opts.attachments) {\n    objectStores.push(ATTACH_STORE);\n  }\n\n  var txnResult = openTransactionSafely(idb, objectStores, 'readonly');\n\n  if (txnResult.error) {\n    return opts.complete(txnResult.error);\n  }\n\n  txn = txnResult.txn;\n  txn.onabort = idbError(opts.complete);\n  txn.oncomplete = onTxnComplete;\n  bySeqStore = txn.objectStore(BY_SEQ_STORE);\n  docStore = txn.objectStore(DOC_STORE);\n  docIdRevIndex = bySeqStore.index('_doc_id_rev');\n  var keyRange = opts.since && !opts.descending ? IDBKeyRange.lowerBound(opts.since, true) : null;\n  runBatchedCursor(bySeqStore, keyRange, opts.descending, limit, onBatch);\n}\n\nvar cachedDBs = new Map();\nvar blobSupportPromise;\nvar openReqList = new Map();\n\nfunction IdbPouch(opts, callback) {\n  var api = this;\n  enqueueTask(function (thisCallback) {\n    init(api, opts, thisCallback);\n  }, callback, api.constructor);\n}\n\nfunction init(api, opts, callback) {\n  var dbName = opts.name;\n  var idb = null;\n  api._meta = null; // called when creating a fresh new database\n\n  function createSchema(db) {\n    var docStore = db.createObjectStore(DOC_STORE, {\n      keyPath: 'id'\n    });\n    db.createObjectStore(BY_SEQ_STORE, {\n      autoIncrement: true\n    }).createIndex('_doc_id_rev', '_doc_id_rev', {\n      unique: true\n    });\n    db.createObjectStore(ATTACH_STORE, {\n      keyPath: 'digest'\n    });\n    db.createObjectStore(META_STORE, {\n      keyPath: 'id',\n      autoIncrement: false\n    });\n    db.createObjectStore(DETECT_BLOB_SUPPORT_STORE); // added in v2\n\n    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {\n      unique: false\n    }); // added in v3\n\n    db.createObjectStore(LOCAL_STORE, {\n      keyPath: '_id'\n    }); // added in v4\n\n    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE, {\n      autoIncrement: true\n    });\n    attAndSeqStore.createIndex('seq', 'seq');\n    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {\n      unique: true\n    });\n  } // migration to version 2\n  // unfortunately \"deletedOrLocal\" is a misnomer now that we no longer\n  // store local docs in the main doc-store, but whaddyagonnado\n\n\n  function addDeletedOrLocalIndex(txn, callback) {\n    var docStore = txn.objectStore(DOC_STORE);\n    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {\n      unique: false\n    });\n\n    docStore.openCursor().onsuccess = function (event) {\n      var cursor = event.target.result;\n\n      if (cursor) {\n        var metadata = cursor.value;\n        var deleted = isDeleted(metadata);\n        metadata.deletedOrLocal = deleted ? \"1\" : \"0\";\n        docStore.put(metadata);\n        cursor.continue();\n      } else {\n        callback();\n      }\n    };\n  } // migration to version 3 (part 1)\n\n\n  function createLocalStoreSchema(db) {\n    db.createObjectStore(LOCAL_STORE, {\n      keyPath: '_id'\n    }).createIndex('_doc_id_rev', '_doc_id_rev', {\n      unique: true\n    });\n  } // migration to version 3 (part 2)\n\n\n  function migrateLocalStore(txn, cb) {\n    var localStore = txn.objectStore(LOCAL_STORE);\n    var docStore = txn.objectStore(DOC_STORE);\n    var seqStore = txn.objectStore(BY_SEQ_STORE);\n    var cursor = docStore.openCursor();\n\n    cursor.onsuccess = function (event) {\n      var cursor = event.target.result;\n\n      if (cursor) {\n        var metadata = cursor.value;\n        var docId = metadata.id;\n        var local = isLocalId$1(docId);\n        var rev = winningRev(metadata);\n\n        if (local) {\n          var docIdRev = docId + \"::\" + rev; // remove all seq entries\n          // associated with this docId\n\n          var start = docId + \"::\";\n          var end = docId + \"::~\";\n          var index = seqStore.index('_doc_id_rev');\n          var range = IDBKeyRange.bound(start, end, false, false);\n          var seqCursor = index.openCursor(range);\n\n          seqCursor.onsuccess = function (e) {\n            seqCursor = e.target.result;\n\n            if (!seqCursor) {\n              // done\n              docStore.delete(cursor.primaryKey);\n              cursor.continue();\n            } else {\n              var data = seqCursor.value;\n\n              if (data._doc_id_rev === docIdRev) {\n                localStore.put(data);\n              }\n\n              seqStore.delete(seqCursor.primaryKey);\n              seqCursor.continue();\n            }\n          };\n        } else {\n          cursor.continue();\n        }\n      } else if (cb) {\n        cb();\n      }\n    };\n  } // migration to version 4 (part 1)\n\n\n  function addAttachAndSeqStore(db) {\n    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE, {\n      autoIncrement: true\n    });\n    attAndSeqStore.createIndex('seq', 'seq');\n    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {\n      unique: true\n    });\n  } // migration to version 4 (part 2)\n\n\n  function migrateAttsAndSeqs(txn, callback) {\n    var seqStore = txn.objectStore(BY_SEQ_STORE);\n    var attStore = txn.objectStore(ATTACH_STORE);\n    var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE); // need to actually populate the table. this is the expensive part,\n    // so as an optimization, check first that this database even\n    // contains attachments\n\n    var req = attStore.count();\n\n    req.onsuccess = function (e) {\n      var count = e.target.result;\n\n      if (!count) {\n        return callback(); // done\n      }\n\n      seqStore.openCursor().onsuccess = function (e) {\n        var cursor = e.target.result;\n\n        if (!cursor) {\n          return callback(); // done\n        }\n\n        var doc = cursor.value;\n        var seq = cursor.primaryKey;\n        var atts = Object.keys(doc._attachments || {});\n        var digestMap = {};\n\n        for (var j = 0; j < atts.length; j++) {\n          var att = doc._attachments[atts[j]];\n          digestMap[att.digest] = true; // uniq digests, just in case\n        }\n\n        var digests = Object.keys(digestMap);\n\n        for (j = 0; j < digests.length; j++) {\n          var digest = digests[j];\n          attAndSeqStore.put({\n            seq: seq,\n            digestSeq: digest + '::' + seq\n          });\n        }\n\n        cursor.continue();\n      };\n    };\n  } // migration to version 5\n  // Instead of relying on on-the-fly migration of metadata,\n  // this brings the doc-store to its modern form:\n  // - metadata.winningrev\n  // - metadata.seq\n  // - stringify the metadata when storing it\n\n\n  function migrateMetadata(txn) {\n    function decodeMetadataCompat(storedObject) {\n      if (!storedObject.data) {\n        // old format, when we didn't store it stringified\n        storedObject.deleted = storedObject.deletedOrLocal === '1';\n        return storedObject;\n      }\n\n      return decodeMetadata(storedObject);\n    } // ensure that every metadata has a winningRev and seq,\n    // which was previously created on-the-fly but better to migrate\n\n\n    var bySeqStore = txn.objectStore(BY_SEQ_STORE);\n    var docStore = txn.objectStore(DOC_STORE);\n    var cursor = docStore.openCursor();\n\n    cursor.onsuccess = function (e) {\n      var cursor = e.target.result;\n\n      if (!cursor) {\n        return; // done\n      }\n\n      var metadata = decodeMetadataCompat(cursor.value);\n      metadata.winningRev = metadata.winningRev || winningRev(metadata);\n\n      function fetchMetadataSeq() {\n        // metadata.seq was added post-3.2.0, so if it's missing,\n        // we need to fetch it manually\n        var start = metadata.id + '::';\n        var end = metadata.id + \"::\\uFFFF\";\n        var req = bySeqStore.index('_doc_id_rev').openCursor(IDBKeyRange.bound(start, end));\n        var metadataSeq = 0;\n\n        req.onsuccess = function (e) {\n          var cursor = e.target.result;\n\n          if (!cursor) {\n            metadata.seq = metadataSeq;\n            return onGetMetadataSeq();\n          }\n\n          var seq = cursor.primaryKey;\n\n          if (seq > metadataSeq) {\n            metadataSeq = seq;\n          }\n\n          cursor.continue();\n        };\n      }\n\n      function onGetMetadataSeq() {\n        var metadataToStore = encodeMetadata(metadata, metadata.winningRev, metadata.deleted);\n        var req = docStore.put(metadataToStore);\n\n        req.onsuccess = function () {\n          cursor.continue();\n        };\n      }\n\n      if (metadata.seq) {\n        return onGetMetadataSeq();\n      }\n\n      fetchMetadataSeq();\n    };\n  }\n\n  api._remote = false;\n\n  api.type = function () {\n    return 'idb';\n  };\n\n  api._id = toPromise(function (callback) {\n    callback(null, api._meta.instanceId);\n  });\n\n  api._bulkDocs = function idb_bulkDocs(req, reqOpts, callback) {\n    idbBulkDocs(opts, req, reqOpts, api, idb, callback);\n  }; // First we look up the metadata in the ids database, then we fetch the\n  // current revision(s) from the by sequence store\n\n\n  api._get = function idb_get(id, opts, callback) {\n    var doc;\n    var metadata;\n    var err;\n    var txn = opts.ctx;\n\n    if (!txn) {\n      var txnResult = openTransactionSafely(idb, [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n\n      txn = txnResult.txn;\n    }\n\n    function finish() {\n      callback(err, {\n        doc: doc,\n        metadata: metadata,\n        ctx: txn\n      });\n    }\n\n    txn.objectStore(DOC_STORE).get(id).onsuccess = function (e) {\n      metadata = decodeMetadata(e.target.result); // we can determine the result here if:\n      // 1. there is no such document\n      // 2. the document is deleted and we don't ask about specific rev\n      // When we ask with opts.rev we expect the answer to be either\n      // doc (possibly with _deleted=true) or missing error\n\n      if (!metadata) {\n        err = createError(MISSING_DOC, 'missing');\n        return finish();\n      }\n\n      var rev;\n\n      if (!opts.rev) {\n        rev = metadata.winningRev;\n        var deleted = isDeleted(metadata);\n\n        if (deleted) {\n          err = createError(MISSING_DOC, \"deleted\");\n          return finish();\n        }\n      } else {\n        rev = opts.latest ? latest(opts.rev, metadata) : opts.rev;\n      }\n\n      var objectStore = txn.objectStore(BY_SEQ_STORE);\n      var key = metadata.id + '::' + rev;\n\n      objectStore.index('_doc_id_rev').get(key).onsuccess = function (e) {\n        doc = e.target.result;\n\n        if (doc) {\n          doc = decodeDoc(doc);\n        }\n\n        if (!doc) {\n          err = createError(MISSING_DOC, 'missing');\n          return finish();\n        }\n\n        finish();\n      };\n    };\n  };\n\n  api._getAttachment = function (docId, attachId, attachment, opts, callback) {\n    var txn;\n\n    if (opts.ctx) {\n      txn = opts.ctx;\n    } else {\n      var txnResult = openTransactionSafely(idb, [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n\n      txn = txnResult.txn;\n    }\n\n    var digest = attachment.digest;\n    var type = attachment.content_type;\n\n    txn.objectStore(ATTACH_STORE).get(digest).onsuccess = function (e) {\n      var body = e.target.result.body;\n      readBlobData(body, type, opts.binary, function (blobData) {\n        callback(null, blobData);\n      });\n    };\n  };\n\n  api._info = function idb_info(callback) {\n    var updateSeq;\n    var docCount;\n    var txnResult = openTransactionSafely(idb, [META_STORE, BY_SEQ_STORE], 'readonly');\n\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n\n    var txn = txnResult.txn;\n\n    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n      docCount = e.target.result.docCount;\n    };\n\n    txn.objectStore(BY_SEQ_STORE).openCursor(null, 'prev').onsuccess = function (e) {\n      var cursor = e.target.result;\n      updateSeq = cursor ? cursor.key : 0;\n    };\n\n    txn.oncomplete = function () {\n      callback(null, {\n        doc_count: docCount,\n        update_seq: updateSeq,\n        // for debugging\n        idb_attachment_format: api._meta.blobSupport ? 'binary' : 'base64'\n      });\n    };\n  };\n\n  api._allDocs = function idb_allDocs(opts, callback) {\n    idbAllDocs(opts, idb, callback);\n  };\n\n  api._changes = function idbChanges(opts) {\n    return changes(opts, api, dbName, idb);\n  };\n\n  api._close = function (callback) {\n    // https://developer.mozilla.org/en-US/docs/IndexedDB/IDBDatabase#close\n    // \"Returns immediately and closes the connection in a separate thread...\"\n    idb.close();\n    cachedDBs.delete(dbName);\n    callback();\n  };\n\n  api._getRevisionTree = function (docId, callback) {\n    var txnResult = openTransactionSafely(idb, [DOC_STORE], 'readonly');\n\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n\n    var txn = txnResult.txn;\n    var req = txn.objectStore(DOC_STORE).get(docId);\n\n    req.onsuccess = function (event) {\n      var doc = decodeMetadata(event.target.result);\n\n      if (!doc) {\n        callback(createError(MISSING_DOC));\n      } else {\n        callback(null, doc.rev_tree);\n      }\n    };\n  }; // This function removes revisions of document docId\n  // which are listed in revs and sets this document\n  // revision to to rev_tree\n\n\n  api._doCompaction = function (docId, revs, callback) {\n    var stores = [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE, ATTACH_AND_SEQ_STORE];\n    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n\n    var txn = txnResult.txn;\n    var docStore = txn.objectStore(DOC_STORE);\n\n    docStore.get(docId).onsuccess = function (event) {\n      var metadata = decodeMetadata(event.target.result);\n      traverseRevTree(metadata.rev_tree, function (isLeaf, pos, revHash, ctx, opts) {\n        var rev = pos + '-' + revHash;\n\n        if (revs.indexOf(rev) !== -1) {\n          opts.status = 'missing';\n        }\n      });\n      compactRevs(revs, docId, txn);\n      var winningRev$$1 = metadata.winningRev;\n      var deleted = metadata.deleted;\n      txn.objectStore(DOC_STORE).put(encodeMetadata(metadata, winningRev$$1, deleted));\n    };\n\n    txn.onabort = idbError(callback);\n\n    txn.oncomplete = function () {\n      callback();\n    };\n  };\n\n  api._getLocal = function (id, callback) {\n    var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readonly');\n\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n\n    var tx = txnResult.txn;\n    var req = tx.objectStore(LOCAL_STORE).get(id);\n    req.onerror = idbError(callback);\n\n    req.onsuccess = function (e) {\n      var doc = e.target.result;\n\n      if (!doc) {\n        callback(createError(MISSING_DOC));\n      } else {\n        delete doc['_doc_id_rev']; // for backwards compat\n\n        callback(null, doc);\n      }\n    };\n  };\n\n  api._putLocal = function (doc, opts, callback) {\n    if (typeof opts === 'function') {\n      callback = opts;\n      opts = {};\n    }\n\n    delete doc._revisions; // ignore this, trust the rev\n\n    var oldRev = doc._rev;\n    var id = doc._id;\n\n    if (!oldRev) {\n      doc._rev = '0-1';\n    } else {\n      doc._rev = '0-' + (parseInt(oldRev.split('-')[1], 10) + 1);\n    }\n\n    var tx = opts.ctx;\n    var ret;\n\n    if (!tx) {\n      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n\n      tx = txnResult.txn;\n      tx.onerror = idbError(callback);\n\n      tx.oncomplete = function () {\n        if (ret) {\n          callback(null, ret);\n        }\n      };\n    }\n\n    var oStore = tx.objectStore(LOCAL_STORE);\n    var req;\n\n    if (oldRev) {\n      req = oStore.get(id);\n\n      req.onsuccess = function (e) {\n        var oldDoc = e.target.result;\n\n        if (!oldDoc || oldDoc._rev !== oldRev) {\n          callback(createError(REV_CONFLICT));\n        } else {\n          // update\n          var req = oStore.put(doc);\n\n          req.onsuccess = function () {\n            ret = {\n              ok: true,\n              id: doc._id,\n              rev: doc._rev\n            };\n\n            if (opts.ctx) {\n              // return immediately\n              callback(null, ret);\n            }\n          };\n        }\n      };\n    } else {\n      // new doc\n      req = oStore.add(doc);\n\n      req.onerror = function (e) {\n        // constraint error, already exists\n        callback(createError(REV_CONFLICT));\n        e.preventDefault(); // avoid transaction abort\n\n        e.stopPropagation(); // avoid transaction onerror\n      };\n\n      req.onsuccess = function () {\n        ret = {\n          ok: true,\n          id: doc._id,\n          rev: doc._rev\n        };\n\n        if (opts.ctx) {\n          // return immediately\n          callback(null, ret);\n        }\n      };\n    }\n  };\n\n  api._removeLocal = function (doc, opts, callback) {\n    if (typeof opts === 'function') {\n      callback = opts;\n      opts = {};\n    }\n\n    var tx = opts.ctx;\n\n    if (!tx) {\n      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n\n      tx = txnResult.txn;\n\n      tx.oncomplete = function () {\n        if (ret) {\n          callback(null, ret);\n        }\n      };\n    }\n\n    var ret;\n    var id = doc._id;\n    var oStore = tx.objectStore(LOCAL_STORE);\n    var req = oStore.get(id);\n    req.onerror = idbError(callback);\n\n    req.onsuccess = function (e) {\n      var oldDoc = e.target.result;\n\n      if (!oldDoc || oldDoc._rev !== doc._rev) {\n        callback(createError(MISSING_DOC));\n      } else {\n        oStore.delete(id);\n        ret = {\n          ok: true,\n          id: id,\n          rev: '0-0'\n        };\n\n        if (opts.ctx) {\n          // return immediately\n          callback(null, ret);\n        }\n      }\n    };\n  };\n\n  api._destroy = function (opts, callback) {\n    changesHandler$1.removeAllListeners(dbName); //Close open request for \"dbName\" database to fix ie delay.\n\n    var openReq = openReqList.get(dbName);\n\n    if (openReq && openReq.result) {\n      openReq.result.close();\n      cachedDBs.delete(dbName);\n    }\n\n    var req = indexedDB.deleteDatabase(dbName);\n\n    req.onsuccess = function () {\n      //Remove open request from the list.\n      openReqList.delete(dbName);\n\n      if (hasLocalStorage() && dbName in localStorage) {\n        delete localStorage[dbName];\n      }\n\n      callback(null, {\n        'ok': true\n      });\n    };\n\n    req.onerror = idbError(callback);\n  };\n\n  var cached = cachedDBs.get(dbName);\n\n  if (cached) {\n    idb = cached.idb;\n    api._meta = cached.global;\n    return nextTick(function () {\n      callback(null, api);\n    });\n  }\n\n  var req = indexedDB.open(dbName, ADAPTER_VERSION);\n  openReqList.set(dbName, req);\n\n  req.onupgradeneeded = function (e) {\n    var db = e.target.result;\n\n    if (e.oldVersion < 1) {\n      return createSchema(db); // new db, initial schema\n    } // do migrations\n\n\n    var txn = e.currentTarget.transaction; // these migrations have to be done in this function, before\n    // control is returned to the event loop, because IndexedDB\n\n    if (e.oldVersion < 3) {\n      createLocalStoreSchema(db); // v2 -> v3\n    }\n\n    if (e.oldVersion < 4) {\n      addAttachAndSeqStore(db); // v3 -> v4\n    }\n\n    var migrations = [addDeletedOrLocalIndex, // v1 -> v2\n    migrateLocalStore, // v2 -> v3\n    migrateAttsAndSeqs, // v3 -> v4\n    migrateMetadata // v4 -> v5\n    ];\n    var i = e.oldVersion;\n\n    function next() {\n      var migration = migrations[i - 1];\n      i++;\n\n      if (migration) {\n        migration(txn, next);\n      }\n    }\n\n    next();\n  };\n\n  req.onsuccess = function (e) {\n    idb = e.target.result;\n\n    idb.onversionchange = function () {\n      idb.close();\n      cachedDBs.delete(dbName);\n    };\n\n    idb.onabort = function (e) {\n      guardedConsole('error', 'Database has a global failure', e.target.error);\n      idb.close();\n      cachedDBs.delete(dbName);\n    }; // Do a few setup operations (in parallel as much as possible):\n    // 1. Fetch meta doc\n    // 2. Check blob support\n    // 3. Calculate docCount\n    // 4. Generate an instanceId if necessary\n    // 5. Store docCount and instanceId on meta doc\n\n\n    var txn = idb.transaction([META_STORE, DETECT_BLOB_SUPPORT_STORE, DOC_STORE], 'readwrite');\n    var storedMetaDoc = false;\n    var metaDoc;\n    var docCount;\n    var blobSupport;\n    var instanceId;\n\n    function completeSetup() {\n      if (typeof blobSupport === 'undefined' || !storedMetaDoc) {\n        return;\n      }\n\n      api._meta = {\n        name: dbName,\n        instanceId: instanceId,\n        blobSupport: blobSupport\n      };\n      cachedDBs.set(dbName, {\n        idb: idb,\n        global: api._meta\n      });\n      callback(null, api);\n    }\n\n    function storeMetaDocIfReady() {\n      if (typeof docCount === 'undefined' || typeof metaDoc === 'undefined') {\n        return;\n      }\n\n      var instanceKey = dbName + '_id';\n\n      if (instanceKey in metaDoc) {\n        instanceId = metaDoc[instanceKey];\n      } else {\n        metaDoc[instanceKey] = instanceId = uuid();\n      }\n\n      metaDoc.docCount = docCount;\n      txn.objectStore(META_STORE).put(metaDoc);\n    } //\n    // fetch or generate the instanceId\n    //\n\n\n    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n      metaDoc = e.target.result || {\n        id: META_STORE\n      };\n      storeMetaDocIfReady();\n    }; //\n    // countDocs\n    //\n\n\n    countDocs(txn, function (count) {\n      docCount = count;\n      storeMetaDocIfReady();\n    }); //\n    // check blob support\n    //\n\n    if (!blobSupportPromise) {\n      // make sure blob support is only checked once\n      blobSupportPromise = checkBlobSupport(txn);\n    }\n\n    blobSupportPromise.then(function (val) {\n      blobSupport = val;\n      completeSetup();\n    }); // only when the metadata put transaction has completed,\n    // consider the setup done\n\n    txn.oncomplete = function () {\n      storedMetaDoc = true;\n      completeSetup();\n    };\n\n    txn.onabort = idbError(callback);\n  };\n\n  req.onerror = function () {\n    var msg = 'Failed to open indexedDB, are you in private browsing mode?';\n    guardedConsole('error', msg);\n    callback(createError(IDB_ERROR, msg));\n  };\n}\n\nIdbPouch.valid = function () {\n  // Following #7085 buggy idb versions (typically Safari < 10.1) are\n  // considered valid.\n  // On Firefox SecurityError is thrown while referencing indexedDB if cookies\n  // are not allowed. `typeof indexedDB` also triggers the error.\n  try {\n    // some outdated implementations of IDB that appear on Samsung\n    // and HTC Android devices <4.4 are missing IDBKeyRange\n    return typeof indexedDB !== 'undefined' && typeof IDBKeyRange !== 'undefined';\n  } catch (e) {\n    return false;\n  }\n};\n\nfunction index(PouchDB) {\n  PouchDB.adapter('idb', IdbPouch, true);\n}\n\nexport default index;","map":null,"metadata":{},"sourceType":"module"}